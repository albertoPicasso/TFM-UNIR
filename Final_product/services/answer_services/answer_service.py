from tools.LLM_tool import LLMTool
from services.answer_services.utils_prompts import UtilsPrompts
from infrastructure.databaseManagers.chroma_database_manager import Chroma_database_manager
from infrastructure.databaseManagers.faiss_database_manager import Faiss_database_manager
from infrastructure.documentLoaders.universal_documents_loader import Universal_documents_loader
from infrastructure.databaseManagers.practise_database_manager import PractiseDatabaseManager
from pathlib import Path
import json
import ast
import logging

class AnswerService ():

    def __init__(self,
                 answer_model_name:str,
                 answer_model_type:str,
                 answer_api_key:str,
                 answer_temperature:float,
                 answer_max_tokens:int,
                 answer_top_k:float,
                 embeddings_model_name:str,
                 database_path:str,
                 content_path:str,
                 database_type:str = "faiss"
                 ):

        self.LLM = LLMTool(
                    model_type=answer_model_type,
                    model_name=answer_model_name,
                    api_key=answer_api_key,
                    temperature=answer_temperature,
                    top_p=answer_top_k,
                    max_tokens=answer_max_tokens)

        self.DATABASE_PATH = database_path
        self.CONTET_PATH = content_path

        if (database_type == "faiss"):
            self.database_manager = Faiss_database_manager(model_name=embeddings_model_name, work_directory = database_path)
        elif(database_type == "chroma"):
            self.database_manager = Chroma_database_manager(model_name=embeddings_model_name, work_directory = database_path)
        else:
            raise ValueError ("Database selected is not implemented")

        self.practise_database_manager = PractiseDatabaseManager(work_directory=database_path, LLM=LLMTool)
        self.dl = Universal_documents_loader(path=self.CONTET_PATH, process_images= False, recursive_mode=False)
        self.logger = logging.getLogger(__name__)




    def regular_answer(self, database_name:str , question:str):
        """
        Generates an answer to a given question using context retrieved from the specified database.

        Parameters:
            database_name (str): The name of the database to retrieve context from.
            question (str): The user's question to be answered.

        Returns:
            str: The response generated by the language model (LLM).

        Description:
            - Retrieves relevant context from the database based on the question.
            - Constructs a prompt combining the question and the context.
            - Sends the prompt to the language model to generate an answer.
        """
        try:
            context = self.database_manager.get_context(query_text=question, database_name=database_name)
            prompt = UtilsPrompts.get_answering_prompt_from_question_and_context(question=question, context=context)
            response = self.LLM.query(prompt=prompt)
            return response
        except Exception as e:
            msg = f"Error en regular_answer: {e}"
            self.logger.error(msg, exc_info=True)
            raise Exception(msg) from e


    def _read_json(self, path):
        """
        Lee un archivo JSON desde el path proporcionado y devuelve su contenido como objeto Python.

        :param path: Ruta al archivo JSON (str o Path).
        :return: Contenido del JSON como dict o list.
        :raises FileNotFoundError: Si el archivo no existe.
        :raises json.JSONDecodeError: Si el contenido no es JSON v√°lido.
        """
        path = Path(path)

        if not path.exists():
            raise FileNotFoundError(f"El archivo no existe: {path}")

        try:
            with path.open("r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            raise json.JSONDecodeError(
                f"Error al decodificar JSON en {path}: {e.msg}", e.doc, e.pos
            )

    def practical_answer (self, database_name:str, folder_name:str, question:str):
        """
        Generates an answer to a practical question by retrieving and using relevant code documents.

        Parameters:
            database_name (str): The name of the JSON summary file containing document metadata.
            folder_name (str): The folder path where the actual documents are stored.
            question (str): The practical question to be answered.

        Returns:
            str: The response generated by the language model (LLM) based on the relevant documents.

        Description:
            - Loads summaries from the specified database to identify relevant documents.
            - Constructs a prompt to query the language model for the most relevant files.
            - Parses the model's response to extract file paths.
            - Loads the content of each relevant document.
            - Constructs a final prompt combining the question and document content.
            - Queries the LLM to generate a practical answer based on the context.
        """


        try:
            # Obtain path of relevant documents
            database_path = self.DATABASE_PATH / database_name
            summaries = self.practise_database_manager.get_context(path=database_path)
            prompt = UtilsPrompts.get_relevant_files_prompt_from_query_and_summaries(query=question, summaries=summaries)
            response = self.LLM.query(prompt=prompt)

            # response = "['carpeta2/bucle_for_solucion_ejercicio_06.py', 'carpeta2/bucle_for_solucion_ejercicio_07.py', 'carpeta2/bucle_for_solucion_ejercicio_08a.py', 'carpeta2/bucle_for_solucion_ejercicio_08b.py']"
            # Load relevant documents
            files = ast.literal_eval(response)
            practise_path = Path(folder_name)
            paths = [practise_path / Path(f) for f in files]
            docs = []

            for p in paths:
                doc = self.dl.load_document(p)
                docs.append(doc)

            # Create answer
            prompt = UtilsPrompts.get_answering_prompt_pratise(query=question, context=docs)
            response = self.LLM.query(prompt=prompt)
            return response

        except Exception as e:
            msg = f"Error procesando la respuesta con documentos relevantes: {e}"
            self.logger.error(msg, exc_info=True)
            raise Exception(msg) from e
